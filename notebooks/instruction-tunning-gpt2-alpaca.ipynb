{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction tunning GPT-2 on Alpaca dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install accelerate transformers datasets trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and dataset configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = os.cpu_count()\n",
    "max_steps = 3000\n",
    "bf16 = False\n",
    "fp16 = False # Turn on if supported\n",
    "fp16_full_eval=False # Turn on if supported\n",
    "gradient_accumulation_steps = 2\n",
    "context_length = 256\n",
    "logging_steps = 500\n",
    "save_steps = 500\n",
    "learning_rate = 0.0001\n",
    "model_name = 'gpt2'\n",
    "out_dir = '../models/gpt2_alpaca_preprocess_fn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Alpaca instructions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b449e451300d4689a1b1d46e2249293e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d49403766ae4592827a91d31836ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14797914248a4f27a17324767e390863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 52002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('tatsu-lab/alpaca')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print one sample from the text column\n",
    "dataset['train']['text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 49401\n",
      "})\n",
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 2601\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "full_dataset = dataset['train'].train_test_split(test_size=0.05, shuffle=True)\n",
    "dataset_train = full_dataset['train']\n",
    "dataset_valid = full_dataset['test']\n",
    " \n",
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset to be used for training GPT-2 model\n",
    "\n",
    "We will use a pre-processing function to prepare the dataset for training the GPT-2 model. The function will concatenate the instruction, input and output columns and formats it into a string with headers 'Instruction', 'Input', and 'Response' for each corresponding value. It returns this structured string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    This function formats the dictionary values into a single string with specific section headers and returns this string.\n",
    "    The returned string is structured as follows:\n",
    "    - Starts with \"### Instruction:\" followed by the instruction value from the dictionary.\n",
    "    - Then \"### Input:\" followed by the input value from the dictionary.\n",
    "    - Finally \"### Response:\" followed by the output value from the dictionary.\n",
    "    Each section is separated by two newline characters for clear demarcation.\n",
    "    \"\"\"\n",
    "    text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nEdit the given text so that it uses formal language.\\n\\n### Input:\\nhey everyone, we need to finish up the project before the weekend\\n\\n### Response:\\nGreetings everyone, we need to complete the project before the weekend.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the preprocess_function\n",
    "preprocess_function(dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the GPT-2 model for instruction tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124,439,808 total parameters.\n",
      "124,439,808 training parameters.\n"
     ]
    }
   ],
   "source": [
    "if bf16:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(dtype=torch.bfloat16)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GPT-2 model on the Alpaca dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{out_dir}/logs\",\n",
    "    evaluation_strategy='steps',\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=logging_steps,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=2,\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "    fp16_full_eval=fp16_full_eval,\n",
    "    report_to='tensorboard',\n",
    "    max_steps=max_steps,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type='constant',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the SFTTrainer\n",
    "We initialize the SFTTrainer with the GPT-2 model, the tokenizer, the training dataset, and the evaluation dataset and the pre-processing function. We also specify `packing=True` which concatenates different samples of similar lengths into one batch to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a59d2249d794acba624c598a890afb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_valid,\n",
    "    max_seq_length=context_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    formatting_func=preprocess_function,\n",
    "    packing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `get_train_dataloader` function of the trainer instance to visualize the concatenated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s * (s-a) * (s-b) * (s-c)) ** 0.5\n",
      "    return area \n",
      "  \n",
      "# Driver Code \n",
      "# a, b, c are lengths of the sides of triangle\n",
      "a = 4\n",
      "b = 5\n",
      "c = 6\n",
      "print(\"Area of triangle is : %.2f\" % area_triangle(a, b, c))<|endoftext|>### Instruction:\n",
      "Generate a headline for an article discussing the importance of backup systems.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\"Don't Wait Until it's Too Late: The Importance of Having a Backup System in Place\"<|endoftext|>### Instruction:\n",
      "Create a speech to inspire people to help the environment.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Good evening everyone. We are all here tonight to recognize the importance of taking care of the environment. We are all so fortunate to live on this beautiful planet and yet, many of us fail to recognize and preserve its beauty. Too often, we take for granted the clean air we breathe, the clean water we drink, and the abundant wildlife that populate our landscapes. It is our moral responsibility to take care of this planet and to ensure that our\n",
      "##################################################\n",
      " networks can take anywhere from a few hours to a few days to train, depending on the size of the data set and the complexity of the task.<|endoftext|>### Instruction:\n",
      "Categorize the following paragraph. Output its categories one by one, seperated by comma.\n",
      "\n",
      "### Input:\n",
      "Two years after the United States ended its combat mission in Afghanistan, violence in the country remains shockingly high, with Taliban insurgents and other insurgent and militant groups maintaining a strong presence across the country. The Taliban has gained ground in some parts of the country, and the Afghan government is struggling to contain the threat.\n",
      "\n",
      "### Response:\n",
      "Afghanistan, Taliban, Military, Government.<|endoftext|>### Instruction:\n",
      "Given a piece of text, identify the tense used.\n",
      "\n",
      "### Input:\n",
      "I would like to have a meeting today.\n",
      "\n",
      "### Response:\n",
      "Present Simple<|endoftext|>### Instruction:\n",
      "Devise an idea for a project that involves using machine learning.\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "An idea for a project involving machine learning would be to develop a system to detect fraudulent credit card transactions. The system would use machine learning algorithms to analyze patterns in the data and identify suspicious transactions.<|endoftext|>### Instruction:\n",
      "Generate a 20-word\n",
      "##################################################\n",
      " ability to access and use computing services from any device with an internet connection has dramatically changed the way we can experience and use technology. Businesses can reduce the costs associated with purchasing and maintaining their own data centers and instead leverage existing cloud computing resources. In addition, cloud computing allows businesses to focus more on generating meaningful insights from data instead of worrying about the underlying infrastructure.\n",
      "\n",
      "Cloud computing also has the advantage of allowing for data storage that is secure, reliable, and available at any time. Companies no longer need to manage each individual piece of hardware and instead can focus on scalability and agility. Cloud computing helps businesses expand without huge investments in hardware and allows IT teams to quickly scale applications to meet the needs of the business.\n",
      "\n",
      "Finally, cloud computing helps companies reduce their impact on the environment as they no longer need to purchase hardware that requires a lot of energy to run. Cloud computing makes it possible to utilize computing power without needing to maintain a hardware infrastructure, saving energy that would otherwise have been expended generating and storing data.<|endoftext|>### Instruction:\n",
      "Create a program that determines the day of the week for a given date.\n",
      "\n",
      "### Input:\n",
      "Date: 5th January, 2021.\n",
      "\n",
      "### Response:\n",
      "def get_day_of_week(date\n",
      "##################################################\n",
      "8 \n",
      "Piece positions:\n",
      "White Rook at H8\n",
      "White Pawn at B3\n",
      "Black Pawn at A3\n",
      "Black Rook at H2\n",
      "\n",
      "### Response:\n",
      "![Image of 8x8 chessboard](https://i.imgur.com/G8HyAoo.png)<|endoftext|>### Instruction:\n",
      "Factor the polynomial x2 + 5x + 6\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The polynomial x2 + 5x + 6 can be factored as (x + 3)(x + 2).<|endoftext|>### Instruction:\n",
      "Generate a series of visual and auditory cues for a time interval of ten seconds\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "For a time interval of ten seconds, a series of visual and auditory cues could include a flashing light combined with a short, repeating audio tone. As the interval progresses, the light could become brighter and the tone could become louder. At the end of the interval, the light could become pulsing and the tone could become slightly deeper.<|endoftext|>### Instruction:\n",
      "Translate the following phrase into German: \"It is a beautiful day.\"\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Es ist ein sch\n",
      "##################################################\n",
      " the crowd. \n",
      "Paragraph 2:\n",
      "Fashion trends come and go, and searching through thrift stores is a great way to stay on top of the current trends without breaking the bank.\n",
      "\n",
      "### Response:\n",
      "Therefore, thrift shopping is beneficial in many ways.<|endoftext|>### Instruction:\n",
      "Reorder the following words to form a question: the take lunch what do you\n",
      "\n",
      "### Input:\n",
      "the take lunch what do you\n",
      "\n",
      "### Response:\n",
      "What do you take for lunch?<|endoftext|>### Instruction:\n",
      "What is the process of photosynthesis and how do plants use it?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Photosynthesis is the process by which plants convert light energy from the Sun into chemical energy that they can use to produce food. In this process, plants use carbon dioxide, water and light energy to create sugar and oxygen. Photosynthesis is a key biological process that all plants need to survive, as it is the primary source of energy for them. The sugar and oxygen produced by this process are essential for the growth and development of plants. Additionally, it leads to the release of oxygen into the atmosphere, which is essential for other life forms.<|endoftext|>### Instruction:\n",
      "Search the web for a valid license for the company\n",
      "##################################################\n",
      " Work Place\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "Technology has had a great impact on the workplace. It has improved productivity, efficiency, and communication. In many instances, it has been used to streamline processes, reduce costs and speed up production. The use of computers, the internet, and other technological tools has enabled companies to become more agile and responsive in the global market. Technology has revolutionized the way companies operate. \n",
      "\n",
      "There have also been some negative effects from technology, though. It can lead to increased privacy and security concerns, decreased face-to-face interaction, and job losses due to automation. However, it has also opened up new job opportunities in areas such as programming, web development, and data analysis. \n",
      "\n",
      "Technology has transformed the workplace in many ways. It has allowed for a greater degree of flexibility in terms of working hours and locations, and has greatly improved the speed and accuracy of data processing. In large organizations, technology allows management to track projects and team performance, as well as to increase efficiency of communication between departments. Technology also enables companies to stay ahead of the competition and keep up with changing market conditions. \n",
      "\n",
      "In conclusion, technology has changed the way the workplace operates. It has allowed organizations to become\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "dataloader = trainer.get_train_dataloader()\n",
    "for i, sample in enumerate(dataloader):\n",
    "    print(tokenizer.decode(sample['input_ids'][0]))\n",
    "    print('#'*50)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c613161d08e242e6beddb93e8ab06560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0502, 'grad_norm': 0.8242108225822449, 'learning_rate': 0.0001, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b6a6d95c4544709cc53ba7c6d2c70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.874091386795044, 'eval_runtime': 120.1069, 'eval_samples_per_second': 7.627, 'eval_steps_per_second': 0.483, 'epoch': 0.9}\n",
      "{'loss': 1.8613, 'grad_norm': 0.7886101603507996, 'learning_rate': 0.0001, 'epoch': 1.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd080def00e3407682cbd613ee1e5445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8360038995742798, 'eval_runtime': 121.6248, 'eval_samples_per_second': 7.531, 'eval_steps_per_second': 0.477, 'epoch': 1.81}\n",
      "{'loss': 1.7669, 'grad_norm': 0.7994106411933899, 'learning_rate': 0.0001, 'epoch': 2.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946f1e15a290474d8f1c79b6eeedf41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8226869106292725, 'eval_runtime': 117.9597, 'eval_samples_per_second': 7.765, 'eval_steps_per_second': 0.492, 'epoch': 2.71}\n",
      "{'loss': 1.6949, 'grad_norm': 0.7560693621635437, 'learning_rate': 0.0001, 'epoch': 3.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e209915b484648a1c13b670ed67eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8207974433898926, 'eval_runtime': 118.9676, 'eval_samples_per_second': 7.7, 'eval_steps_per_second': 0.488, 'epoch': 3.62}\n",
      "{'loss': 1.6329, 'grad_norm': 0.7565377950668335, 'learning_rate': 0.0001, 'epoch': 4.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513dcfa180614c78a041c6a39ce2cd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8262083530426025, 'eval_runtime': 120.393, 'eval_samples_per_second': 7.608, 'eval_steps_per_second': 0.482, 'epoch': 4.52}\n",
      "{'loss': 1.5768, 'grad_norm': 0.7407647967338562, 'learning_rate': 0.0001, 'epoch': 5.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c432de3149be4b0b9cf27d1721cebbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8371939659118652, 'eval_runtime': 119.4518, 'eval_samples_per_second': 7.668, 'eval_steps_per_second': 0.486, 'epoch': 5.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8208.0386, 'train_samples_per_second': 11.696, 'train_steps_per_second': 0.365, 'train_loss': 1.7638329671223958, 'epoch': 5.42}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          global_step  training_loss       metrics\n",
      "train_runtime                    3000       1.763833  8.208039e+03\n",
      "train_samples_per_second         3000       1.763833  1.169600e+01\n",
      "train_steps_per_second           3000       1.763833  3.650000e-01\n",
      "total_flos                       3000       1.763833  1.253483e+16\n",
      "train_loss                       3000       1.763833  1.763833e+00\n",
      "epoch                            3000       1.763833  5.424955e+00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "history_dict = history._asdict()\n",
    "history_df = pd.DataFrame(history_dict)\n",
    "print(history_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/gpt2_alpaca_preprocess_fn/best_model/tokenizer_config.json',\n",
       " '../models/gpt2_alpaca_preprocess_fn/best_model/special_tokens_map.json',\n",
       " '../models/gpt2_alpaca_preprocess_fn/best_model/vocab.json',\n",
       " '../models/gpt2_alpaca_preprocess_fn/best_model/merges.txt',\n",
       " '../models/gpt2_alpaca_preprocess_fn/best_model/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(f\"{out_dir}/best_model\")\n",
    "tokenizer.save_pretrained(f\"{out_dir}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the instruction tuned GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    logging, \n",
    "    pipeline,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(f'{out_dir}/best_model/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'{out_dir}/best_model/')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline to generate text\n",
    "pipe = pipeline(\n",
    "    task='text-generation', \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=256, # Prompt + new tokens to generate.\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Write a sentence using the given context\n",
      "### Input:\n",
      "dog, beach, sun\n",
      "### Response:\n",
      "The sun shone down on the dog's face as it watched the sand below.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt using the same format in which the model was trained\n",
    "template = \"\"\"### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Write the prompt\n",
    "instructions = 'Write a sentence using the given context'\n",
    "inputs = 'dog, beach, sun'\n",
    "response = ''\n",
    "prompt = template.format(instructions, inputs, response)\n",
    "\n",
    "# Generate text\n",
    "outputs = pipe(\n",
    "    prompt, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not bad! The model is able to generate the correct output for the given instruction.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have trained the GPT-2 model on the Alpaca dataset using the SFTTrainer. We have also tested the model on a sample instruction to check if it is able to generate the correct output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
